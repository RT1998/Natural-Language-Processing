{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization: Explained with Simple Examples\n",
        "\n",
        "### What is Tokenization?\n",
        "\n",
        "Tokenization is the process of converting a stream of text into individual words, sentences, or subwords (tokens). This is an essential step in Natural Language Processing (NLP) as it breaks down text into manageable units for further analysis.\n",
        "\n",
        "**2. Types of Tokenization**\n",
        "\n",
        "    Word Tokenization: Splitting the text into individual words.\n",
        "    Sentence Tokenization: Splitting the text into individual sentences.\n",
        "\n",
        "**Why It's Useful:**  \n",
        "Think of a sentence as a big puzzle. Tokenization helps us by breaking it down into smaller pieces (tokens), making it easier to analyze and understand each piece.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nCxjwvx3QfOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#en_core_web_sm for English text processing."
      ],
      "metadata": {
        "id": "ImsEjLcFQhcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "text = \"I love Natural Language Processing (NLP). I am using Spacy for it\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize words\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLPL5Kf7RS7T",
        "outputId": "27c8f7bf-00df-4734-b431-704221a646b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'I', 'am', 'using', 'Spacy', 'for', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize sentence\n",
        "text = \"I love Natural Language Processing (NLP). I am using Spacy for it\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize sentence\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCt1HfNqRZlf",
        "outputId": "7c625a8b-eb29-452c-8a1e-9041292809fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love Natural Language Processing (NLP).', 'I am using Spacy for it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Only Words Without Punctuation\n",
        "text = \"I love Natural Language Processing (NLP). I am using Spacy for it\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize only words (without punctuation)\n",
        "tokens_without_punctuation = [token.text for token in doc if token.is_alpha]\n",
        "print(tokens_without_punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4kiHG4kRl7e",
        "outputId": "7d4c8926-b3cc-41bf-9e9c-ee2e23afa016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'Natural', 'Language', 'Processing', 'NLP', 'I', 'am', 'using', 'Spacy', 'for', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Without Punctuation and Numbers\n",
        "\n",
        "text = \"The price of the book is $50  and the tax is $5.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens_without_punctuation_and_numbers = [token.text for token in doc if token.is_alpha]\n",
        "print(tokens_without_punctuation_and_numbers)\n",
        "\n",
        "# Tokenize with words and punctuation without numbers\n",
        "tokens_without_numbers = [token.text for token in doc if not token.is_digit]\n",
        "print(tokens_without_numbers)\n",
        "\n",
        "# Tokenize with words and numbers without punctuation\n",
        "tokens_without_punctuation = [token.text for token in doc if not token.is_punct]\n",
        "print(tokens_without_punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wznh5Dk5RyJj",
        "outputId": "b2e50755-6e50-4421-c6d5-cb9f286d19b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'the', 'book', 'is', 'and', 'the', 'tax', 'is']\n",
            "['The', 'price', 'of', 'the', 'book', 'is', '$', ' ', 'and', 'the', 'tax', 'is', '$', '.']\n",
            "['The', 'price', 'of', 'the', 'book', 'is', '$', '50', ' ', 'and', 'the', 'tax', 'is', '$', '5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing Words with Apostrophes\n",
        "\n",
        "text = \"I'm learning NLP and it's amazing!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize words with apostrophes\n",
        "tokens_with_apostrophes = [token.text for token in doc]\n",
        "print(tokens_with_apostrophes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Selu5-R0sl",
        "outputId": "66b668ac-e149-424d-d91b-05428738b23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'learning', 'NLP', 'and', 'it', \"'s\", 'amazing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Text\n",
        "#Tokenize Only Phone Numbers\n",
        "text = \"Call me at 123-456-7890 or 987-654-3210.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize only phone numbers (looking for patterns like digits and hyphens)\n",
        "tokens_with_phone_numbers = [token.text for token in doc if token.like_num]\n",
        "print(tokens_with_phone_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxgrqIU-SkgE",
        "outputId": "4d1dcad5-8ebf-48b9-ec2f-452ba52faeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['123', '456', '7890', '987', '654', '3210']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "# using re for this\n",
        "\n",
        "# Example with a phone number\n",
        "text = \"Call me at 123-456-7890 or 987-654-3210.\"\n",
        "pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
        "\n",
        "# Tokenize using regex to capture phone numbers\n",
        "tokens_with_phone_numbers = [match.group() for match in re.finditer(pattern, text)]\n",
        "print(tokens_with_phone_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRCiEB9NTKKT",
        "outputId": "58a67920-6be5-42ab-bc49-89228cc50bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['123-456-7890', '987-654-3210']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_text = \"Hello world! ðŸ˜Š I'm loving #NLP and it's so fun! @user\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(tweet_text)\n",
        "\n",
        "tokens_tweet = [token.text for token in doc]\n",
        "print(tokens_tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPGWfJ2GS7s-",
        "outputId": "055fc3ea-3338-4d7a-8879-883595c8945b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world', '!', 'ðŸ˜Š', 'I', \"'m\", 'loving', '#', 'NLP', 'and', 'it', \"'s\", 'so', 'fun', '!', '@user']\n"
          ]
        }
      ]
    }
  ]
}