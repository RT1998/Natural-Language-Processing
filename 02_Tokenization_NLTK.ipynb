{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization: Explained with Simple Examples\n",
        "\n",
        "### What is Tokenization?\n",
        "\n",
        "Tokenization is the process of converting a stream of text into individual words, sentences, or subwords (tokens). This is an essential step in Natural Language Processing (NLP) as it breaks down text into manageable units for further analysis.\n",
        "\n",
        "**2. Types of Tokenization**\n",
        "\n",
        "    Word Tokenization: Splitting the text into individual words.\n",
        "    Sentence Tokenization: Splitting the text into individual sentences.\n",
        "\n",
        "**Why It's Useful:**  \n",
        "Think of a sentence as a big puzzle. Tokenization helps us by breaking it down into smaller pieces (tokens), making it easier to analyze and understand each piece.\n"
      ],
      "metadata": {
        "id": "1OD5xRkXKZ91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#The \"punkt\" model in NLTK is an essential tool for tokenization in NLP tasks.\n",
        "#It is pre-trained on text data and can effectively split text into sentences and words in various languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_w8vPN9MiIq",
        "outputId": "e57f0f30-5289-456e-a473-684c45274099"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAH6UivXJjQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c5ea7f-bdba-41ed-fa69-e1299bcb921e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
            "number of items in list :  8\n"
          ]
        }
      ],
      "source": [
        "#Word Tokenization with NLTK\n",
        "# NLTKâ€™s word_tokenize() function is already quite good at handling special cases like contractions, punctuation, and symbols.\n",
        "\n",
        "#example use\n",
        "text = \"Hello, how are you doing today?\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(words)\n",
        "print(\"number of items in list : \", len(words))\n",
        "\n",
        "\n",
        "text = \"I'm excited to go to the market! It's going to be fun.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "print(\"number of items in list : \", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization with NLTK\n",
        "# Example text\n",
        "text = \"Hello, how are you doing today? I hope you're doing well.\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(\"number of items in list : \", len(sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtBOMd8YMRhx",
        "outputId": "99890ccd-5bb3-4a51-8ce4-5d2e52284f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello, how are you doing today?', \"I hope you're doing well.\"]\n",
            "number of items in list :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Customizing Tokenization\n",
        "\n",
        "# The custom tokenizer uses a regular expression r'\\w+|[^\\w\\s]' which matches words (\\w+) and punctuation marks ([^\\w\\s]).\n",
        "# This allows for the separation of punctuation from words, making it especially useful in text analysis tasks where punctuation marks carry significan\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Define a tokenizer to extract only words (ignoring punctuation)\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, how are you doing today?\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[^\\w\\s]+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Tokenizing only numbers\n",
        "tokenizer = RegexpTokenizer(r'\\d+')\n",
        "text = \"The price of the book is 50 dollars and the tax is 5.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "## Tokenizing decimal numbers\n",
        "tokenizer = RegexpTokenizer(r'\\d+\\.\\d+')\n",
        "text = \"The price is 12.99 dollars and the discount is 0.25.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Tokenizing numbers with commas\n",
        "tokenizer = RegexpTokenizer(r'\\d{1,3}(?:,\\d{3})*')\n",
        "text = \"The population is 1,234,567 and the revenue is 987,654,321.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Tokenizing percentages\n",
        "tokenizer = RegexpTokenizer(r'\\d+%')\n",
        "text = \"The success rate was 75% last year, but now it is 80%.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Tokenizing phone numbers\n",
        "tokenizer = RegexpTokenizer(r'\\+?\\d{1,2}\\s?\\(?\\d{1,4}\\)?[\\s\\-]?\\d{1,4}[\\s\\-]?\\d{1,4}')\n",
        "text = \"You can reach me at 800-555-1234 or +44 20 7946 0958.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btsUbAPXMzT8",
        "outputId": "3eddc062-dec6-40a5-bb0c-73d8d4e598c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'how', 'are', 'you', 'doing', 'today']\n",
            "[',', '?']\n",
            "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
            "['50', '5']\n",
            "['12.99', '0.25']\n",
            "['1,234,567', '987,654,321']\n",
            "['75%', '80%']\n",
            "['800-555-1234', '+44 20 7946 0958']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture only words (ignores numbers): [A-Za-z]+\n",
        "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
        "text = \"The price of the book is 50 dollars and the tax is 5.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Capture words and punctuation (ignores numbers): [A-Za-z]+|[.,!?;]\n",
        "tokenizer = RegexpTokenizer(r'[A-Za-z]+|[.,!?;]')\n",
        "text = \"The price of the book is 50 dollars, and the tax is 5!\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Capture words with apostrophes (ignores numbers and phone numbers): [A-Za-z]+(?:'[A-Za-z]+)\n",
        "tokenizer = RegexpTokenizer(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "text = \"I'm looking forward to seeing 123-456-7890, it's a great day!\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRzZyc-3PWKl",
        "outputId": "498d90b0-bbdc-418d-a39e-46962e1a61e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'the', 'book', 'is', 'dollars', 'and', 'the', 'tax', 'is']\n",
            "['The', 'price', 'of', 'the', 'book', 'is', 'dollars', ',', 'and', 'the', 'tax', 'is', '!']\n",
            "[\"I'm\", 'looking', 'forward', 'to', 'seeing', \"it's\", 'a', 'great', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Hello! My name is Alice. I work at the XYZ company.\n",
        "I love programming in Python. How about you?\"\"\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Word tokenization for each sentence\n",
        "word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "print(word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FtZpjk6M9e-",
        "outputId": "06254a4f-9dd3-47f3-c6eb-6a905539d7ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hello', '!'], ['My', 'name', 'is', 'Alice', '.'], ['I', 'work', 'at', 'the', 'XYZ', 'company', '.'], ['I', 'love', 'programming', 'in', 'Python', '.'], ['How', 'about', 'you', '?']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# he TweetTokenizer in NLTK is a specialized tokenizer designed specifically for tokenizing text from social media, particularly tweets. It handles the unique characteristics of social media text, such as hashtags, mentions, and emoticons, more effectively than a regular tokenizer.\n",
        "# This tokenizer is optimized to deal with the noisy,\n",
        "# informal language that is typical in social media content.\n",
        "\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Initialize the TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "# Example tweet text\n",
        "tweet_text = \"Hello world! ðŸ˜Š I'm loving #NLP and it's so fun! @user\"\n",
        "\n",
        "# Tokenizing the tweet\n",
        "tokens = tweet_tokenizer.tokenize(tweet_text)\n",
        "\n",
        "# Displaying the tokens\n",
        "print(\"Tokens from tweet:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rStQuoGmNyhr",
        "outputId": "981c04b9-3c11-40ab-eb4b-9e2bfe4e8c92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens from tweet:\n",
            "['Hello', 'world', '!', 'ðŸ˜Š', \"I'm\", 'loving', '#NLP', 'and', \"it's\", 'so', 'fun', '!', '@user']\n"
          ]
        }
      ]
    }
  ]
}