{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Stemming\n",
        "\n",
        "**Definition**:  \n",
        "Stemming is the process of reducing a word to its base or root form by removing prefixes or suffixes. It typically doesn‚Äôt consider the meaning or context of the word, so it may result in non-dictionary forms (known as \"stems\").\n",
        "\n",
        "**Purpose**:  \n",
        "Stemming simplifies words to their root forms, which can help improve performance in tasks like text classification, information retrieval, and other NLP applications.\n",
        "\n",
        "### Example:\n",
        "- \"running\" ‚Üí \"run\"\n",
        "- \"flies\" ‚Üí \"fli\" (non-dictionary form)\n",
        "\n",
        "\n",
        "## 2. Lemmatization\n",
        "\n",
        "**Definition**:  \n",
        "Lemmatization is a more sophisticated approach that reduces words to their lemma (the dictionary form of the word). Unlike stemming, lemmatization considers the word's meaning and context to ensure the transformation results in a valid word.\n",
        "\n",
        "**Purpose**:  \n",
        "Lemmatization ensures that words with different forms (e.g., \"running\", \"ran\", \"runs\") are reduced to a common base word (\"run\"), while maintaining their meaning.\n",
        "\n",
        "### Example:\n",
        "- \"running\" ‚Üí \"run\" (valid word in the dictionary)\n",
        "- \"flies\" ‚Üí \"fly\" (valid word in the dictionary)"
      ],
      "metadata": {
        "id": "wRBt3OxsVyZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "# Initialize stemmers\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Words to be stemmed\n",
        "words = [\"running\", \"flies\", \"universities\", \"better\"]\n",
        "\n",
        "# Apply stemming using different stemmers\n",
        "print(\"Porter Stemmer:\", [porter.stem(word) for word in words])\n",
        "print(\"Snowball Stemmer:\", [snowball.stem(word) for word in words])\n",
        "print(\"Lancaster Stemmer:\", [lancaster.stem(word) for word in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tROCARapW7kJ",
        "outputId": "221c47b9-8fa1-4e79-d8b3-3a4dfa775cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['run', 'fli', 'univers', 'better']\n",
            "Snowball Stemmer: ['run', 'fli', 'univers', 'better']\n",
            "Lancaster Stemmer: ['run', 'fli', 'univers', 'bet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq5BrPZZXgRM",
        "outputId": "a1723ef2-07e5-470b-e1d7-a57e6c49d334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Words to be lemmatized\n",
        "words = [\"running\", \"flies\", \"better\"]\n",
        "print(\"Lemmatized Words (default POS 'v'):\", [lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The cats are running quickly.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# POS tagging\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Lemmatize with proper POS tagging\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v' if tag.startswith('V') else 'n') for word, tag in tagged]\n",
        "print(\"Lemmatized with POS:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFlJv-44XGAu",
        "outputId": "bf9d4389-5239-4e47-a167-a047919b569c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words (default POS 'v'): ['run', 'fly', 'better']\n",
            "Lemmatized with POS: ['The', 'cat', 'be', 'run', 'quickly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#custom lemmatizer NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example tweet\n",
        "tweet = \"LOL! I'm gonna be BRB, IDK if I can make it #excited #NLP\"\n",
        "\n",
        "# Custom slang dictionary\n",
        "slang_dict = {\n",
        "    \"lol\": \"laughing_out_loud\",\n",
        "    \"brb\": \"be_right_back\",\n",
        "    \"idk\": \"i_dont_know\",\n",
        "    \"gonna\": \"going_to\"\n",
        "}\n",
        "\n",
        "tokens = nltk.word_tokenize(tweet)\n",
        "lemmatized_tweet = [slang_dict.get(token.lower(), lemmatizer.lemmatize(token)) for token in tokens]\n",
        "print(\"Custom Lemmatized Tweet:\", lemmatized_tweet)\n",
        "\n",
        "tokens = nltk.word_tokenize(tweet)\n",
        "lemmatized_tweet = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized:\", lemmatized_tweet)\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v' if tag.startswith('V') else 'n') for word, tag in tagged]\n",
        "print(\"Lemmatized:\", lemmatized_tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWtCFry5YAN_",
        "outputId": "1eb3fe64-2e63-4162-ecf8-13f71c740189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Lemmatized Tweet: ['laughing_out_loud', '!', 'I', \"'m\", 'gon', 'na', 'be', 'be_right_back', ',', 'i_dont_know', 'if', 'I', 'can', 'make', 'it', '#', 'excited', '#', 'NLP']\n",
            "Lemmatized: ['LOL', '!', 'I', \"'m\", 'gon', 'na', 'be', 'BRB', ',', 'IDK', 'if', 'I', 'can', 'make', 'it', '#', 'excited', '#', 'NLP']\n",
            "Lemmatized: ['LOL', '!', 'I', \"'m\", 'gon', 'na', 'be', 'BRB', ',', 'IDK', 'if', 'I', 'can', 'make', 'it', '#', 'excited', '#', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy Stemming:\n",
        "# spaCy does not perform stemming by default\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The cats were running quickly.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Lemmatized words\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vZItdnTve2T",
        "outputId": "93e22934-cc19-4c10-9f18-fbe5817dbdb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['the', 'cat', 'be', 'run', 'quickly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "tweet = \"LOL! I'm gonna be BRB, IDK if I can make it #excited #NLP\"\n",
        "# Custom slang dictionary for spaCy\n",
        "slang_dict = {\n",
        "    \"lol\": \"laughing_out_loud\",\n",
        "    \"brb\": \"be_right_back\",\n",
        "    \"idk\": \"i_dont_know\",\n",
        "    \"gonna\": \"going_to\"\n",
        "}\n",
        "\n",
        "doc = nlp(tweet)\n",
        "lemmatized_tweet_spacy = [slang_dict.get(token.text.lower(), token.lemma_) for token in doc]\n",
        "print(\"Custom Lemmatized Tweet (spaCy):\", lemmatized_tweet_spacy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOsJ7mIpYCs9",
        "outputId": "c67186cf-0e83-4404-8e44-49f47ded8b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Lemmatized Tweet (spaCy): ['laughing_out_loud', '!', 'I', 'be', 'go', 'to', 'be', 'be_right_back', ',', 'i_dont_know', 'if', 'I', 'can', 'make', 'it', '#', 'excited', '#', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_epEYdUaMPt",
        "outputId": "0125ceba-949b-4b90-974b-b278d9bb0fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Function to custom tokenize hashtags and mentions\n",
        "def custom_tokenize(text):\n",
        "    # Replace hashtags and mentions with single tokens\n",
        "    text = re.sub(r'#\\w+', lambda x: x.group(), text)  # Keep hashtags intact\n",
        "    text = re.sub(r'@\\w+', lambda x: x.group(), text)  # Keep mentions intact\n",
        "    return text\n",
        "\n",
        "# Example tweet\n",
        "tweet2 = \"I'm loving #AI and #MachineLearning, follow me @user!\"\n",
        "\n",
        "# Custom tokenize the tweet\n",
        "custom_tokenized_tweet = custom_tokenize(tweet2)\n",
        "print(custom_tokenized_tweet)\n",
        "\n",
        "# Lemmatize the tweet with\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens2 = word_tokenize(custom_tokenized_tweet)\n",
        "lemmatized_tweet2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "print(\"Custom Tokenized and Lemmatized Tweet (NLTK):\", lemmatized_tweet2)\n",
        "\n",
        "\n",
        "# Lemmatize the tweet with spaCy\n",
        "doc2 = nlp(custom_tokenized_tweet)\n",
        "lemmatized_tweet2_spacy = [token.lemma_ for token in doc2]\n",
        "print(\"Custom Tokenized and Lemmatized Tweet (spaCy):\", lemmatized_tweet2_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jlIqswrarid",
        "outputId": "fbca57fe-7483-4a57-fd4e-480aa75c0e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm loving #AI and #MachineLearning, follow me @user!\n",
            "Custom Tokenized and Lemmatized Tweet (NLTK): ['I', \"'m\", 'loving', '#', 'AI', 'and', '#', 'MachineLearning', ',', 'follow', 'me', '@', 'user', '!']\n",
            "Custom Tokenized and Lemmatized Tweet (spaCy): ['I', 'be', 'love', '#', 'AI', 'and', '#', 'MachineLearning', ',', 'follow', 'I', '@us', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# Example tweet with emojis\n",
        "tweet = \"I love NLP üòä and it's so fun! üòÑ #AI\"\n",
        "\n",
        "# Function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    return re.sub(r'[^\\w\\s,]', '', text)\n",
        "\n",
        "# Tokenize tweet\n",
        "tokens = nltk.word_tokenize(remove_emojis(tweet))\n",
        "print(\"Tokens after Removing Emojis:\", tokens)\n",
        "\n",
        "\n",
        "import spacy\n",
        "import emoji\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example tweet with emojis\n",
        "tweet = \"I love NLP üòä and it's so fun! üòÑ #AI\"\n",
        "\n",
        "# Function to replace emojis with mapped text\n",
        "def map_emoji_to_text(text):\n",
        "    emoji_dict = {'üòä': 'happy', 'üòÑ': 'joyful'}\n",
        "    for emj, word in emoji_dict.items():\n",
        "        text = text.replace(emj, word)\n",
        "    return text\n",
        "\n",
        "# Process the tweet with spaCy\n",
        "custom_tweet = map_emoji_to_text(tweet)\n",
        "doc = nlp(custom_tweet)\n",
        "\n",
        "# Tokenize tweet\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Tokens after Mapping Emojis to Text:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKMCYrI5aGRI",
        "outputId": "eb337ef3-e6cd-400f-ebcd-644ca6384ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after Removing Emojis: ['I', 'love', 'NLP', 'and', 'its', 'so', 'fun', 'AI']\n",
            "Tokens after Mapping Emojis to Text: ['I', 'love', 'NLP', 'happy', 'and', 'it', \"'s\", 'so', 'fun', '!', 'joyful', '#', 'AI']\n"
          ]
        }
      ]
    }
  ]
}